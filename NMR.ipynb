{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install numerapi\n",
    "!pip3 install livelossplot\n",
    "!pip3 install pydot\n",
    "!apt install -y graphviz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numerapi\n",
    "example_public_id  = \"XXX\"\n",
    "example_secret_key = \"YYY\"\n",
    "napi               = numerapi.NumerAPI(example_public_id, example_secret_key)\n",
    "SESSION            = 'bernie'\n",
    "datasetname        = napi.download_current_dataset(unzip=True).replace('.zip', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "training        = pd.read_csv(datasetname+'/numerai_training_data.csv')\n",
    "tournament      = pd.read_csv(datasetname+'/numerai_tournament_data.csv', header=0, index_col=None)\n",
    "validation_data = tournament[tournament.data_type=='validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tournament.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "    training shape: {}\n",
    "    tournament shape: {}\"\"\".format(training.shape, tournament.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [f for f in list(training) if 'feature' in f]\n",
    "X        = training[features].values\n",
    "X        = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "\n",
    "all_sessions = ['bernie', 'charles', 'elizabeth', 'jordan', 'ken']\n",
    "for s in all_sessions:\n",
    "    globals()['Y_{}'.format(s)] = training['target_{}'.format(s)].values\n",
    "\n",
    "Y = globals()['Y_'+SESSION]\n",
    "\n",
    "validation_data = tournament[tournament.data_type=='validation']\n",
    "valX            = validation_data[features].values\n",
    "valX            = valX.reshape(valX.shape[0], 1, valX.shape[1])\n",
    "\n",
    "for s in all_sessions:\n",
    "    globals()['valY_{}'.format(s)] = validation_data['target_{}'.format(s)].values\n",
    "\n",
    "valY = globals()['valY_'+SESSION]\n",
    "    \n",
    "testX = tournament[features].values\n",
    "testX = testX.reshape(testX.shape[0], 1, testX.shape[1])\n",
    "ids   = tournament['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, Y_bernie.shape, valX.shape, valY_bernie.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature sorting by information gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "regr = RandomForestRegressor(max_depth=7, random_state=42)\n",
    "regr.fit(X.reshape(X.shape[0], X.shape[2]), Y)\n",
    "\n",
    "feature_gains = {}\n",
    "for i, f in enumerate(regr.feature_importances_):\n",
    "    feature_gains[i] = f\n",
    "\n",
    "sorted_feature_indexes = list(map(lambda x: x[0], sorted(feature_gains.items(), key=lambda kv: kv[1], reverse=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X    = X[:,:,np.argsort(sorted_feature_indexes)].copy()\n",
    "valX = valX[:,:,np.argsort(sorted_feature_indexes)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.layers import Input, Conv1D, LSTM, AveragePooling1D, AlphaDropout\n",
    "from livelossplot import PlotLossesKeras\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.initializers import lecun_normal\n",
    "from AdamW import AdamW\n",
    "from keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timewindow  = X.shape[1]\n",
    "numfeatures = X.shape[2]\n",
    "\n",
    "inputs   = Input(shape=(timewindow, numfeatures), name='input')\n",
    "x        = inputs\n",
    "\n",
    "x  = Dense(128, activation=\"relu\", name=\"encoderlayer1\", kernel_initializer=lecun_normal())(x)\n",
    "x  = Dense(64,  activation=\"relu\", name=\"encoderlayer2\", kernel_initializer=lecun_normal())(x)\n",
    "\n",
    "x  = Dense(32,  activation=\"relu\", name=\"encoder\", kernel_initializer=lecun_normal())(x)\n",
    "\n",
    "x  = Dense(64,  activation=\"relu\", name=\"decoderlayer1\", kernel_initializer=lecun_normal())(x)\n",
    "x  = Dense(128, activation=\"relu\", name=\"decoderlayer2\", kernel_initializer=lecun_normal())(x)\n",
    "x  = Dense(numfeatures, activation='sigmoid', name=\"decoder\", kernel_initializer=lecun_normal())(x)\n",
    "\n",
    "\n",
    "aemodel = Model(inputs=inputs, outputs=x)\n",
    "aemodel.summary()\n",
    "\n",
    "from keras.utils import plot_model\n",
    "plot_model(aemodel, to_file='model.png')\n",
    "from IPython.display import Image\n",
    "Image(url= \"model.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "epochs     = 100\n",
    "\n",
    "b, B, T = batch_size, X.shape[0], epochs\n",
    "wd = 0.005 * (b/B/T)**0.5\n",
    "\n",
    "aemodel.compile(loss='mean_squared_error',  optimizer=AdamW(weight_decay=wd), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_histories = []\n",
    "for i in range(5):\n",
    "    noise_factor = 0.00001\n",
    "    noisyX = X + noise_factor * np.random.normal(loc=0.0, scale=1, size=X.shape)\n",
    "    noisyvalX = valX + noise_factor * np.random.normal(loc=0.0, scale=1, size=valX.shape)\n",
    "\n",
    "    ae_histories.append(\n",
    "        aemodel.fit(noisyX, X, batch_size=batch_size, epochs=epochs, shuffle=True, \n",
    "                    callbacks=[PlotLossesKeras()], \n",
    "                    validation_data=(noisyvalX, valX)))\n",
    "    \n",
    "ae_histories.append(\n",
    "    aemodel.fit(X, X, batch_size=batch_size, epochs=epochs, shuffle=False, \n",
    "                callbacks=[PlotLossesKeras()], \n",
    "                validation_data=(valX, valX)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, val_loss = [], []\n",
    "for h in ae_histories:\n",
    "    train_loss += h.history['loss']\n",
    "    val_loss   += h.history['val_loss']\n",
    "plt.plot(train_loss)\n",
    "plt.plot(val_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_model = Model(inputs=aemodel.input, outputs=aemodel.get_layer(\"encoder\").output)\n",
    "compressed_X     = compressed_model.predict(X)                                                                                                             \n",
    "compressed_valX  = compressed_model.predict(valX)\n",
    "compressed_testX = compressed_model.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timewindow  = compressed_X.shape[1]\n",
    "numfeatures = compressed_X.shape[2]\n",
    "\n",
    "inputs   = Input(shape=(timewindow, numfeatures))\n",
    "x        = inputs\n",
    "\n",
    "x  = Conv1D(filters=128, kernel_size=3, padding='same', activation='relu', name='convfeatures1')(x)\n",
    "x  = AveragePooling1D(pool_size=2, padding='same')(x)\n",
    "\n",
    "x  = Dense(256, activation='selu', kernel_initializer=lecun_normal())(x)\n",
    "x  = AlphaDropout(0.1)(x)\n",
    "x  = Dense(128, activation='selu', kernel_initializer=lecun_normal())(x)\n",
    "x  = AlphaDropout(0.1)(x)\n",
    "x  = Dense(64, activation='selu', kernel_initializer=lecun_normal())(x)\n",
    "x  = AlphaDropout(0.1)(x)\n",
    "\n",
    "x  = Flatten()(x)\n",
    "x  = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "classifier = Model(inputs=inputs, outputs=x)\n",
    "classifier.summary()\n",
    "\n",
    "from keras.utils import plot_model\n",
    "plot_model(classifier, to_file='classifier.png')\n",
    "from IPython.display import Image\n",
    "Image(url= \"classifier.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "epochs     = 100\n",
    "\n",
    "b, B, T = batch_size, X.shape[0], epochs\n",
    "wd = 0.005 * (b/B/T)**0.5\n",
    "\n",
    "classifier.compile(loss='binary_crossentropy', \n",
    "                          optimizer=AdamW(weight_decay=wd),\n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "REDUCE_LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-5)\n",
    "\n",
    "!rm -rf /tmp/classifier-c-s-g\n",
    "history = classifier.fit(compressed_X, Y, batch_size=batch_size, epochs=epochs, shuffle=True, \n",
    "                             callbacks=[PlotLossesKeras(), REDUCE_LR], \n",
    "                             validation_data=(compressed_valX, valY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = classifier.predict(compressed_testX)\n",
    "pred = pred.reshape(pred.shape[0])\n",
    "print (\"min: {}, max: {}\".format(pred.min(), pred.max()))\n",
    "results_df  = pd.DataFrame(data={'probability_bernie': pred})\n",
    "joined      = pd.DataFrame(ids).join(results_df)\n",
    "joined.to_csv('predictions_{}.csv'.format(SESSION), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_id = napi.upload_predictions(\"predictions_bernie.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "napi.submission_status()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
